# -*- coding: utf-8 -*-
"""VisionTransformersInsulatorClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zk1lR32e7ieKngAFEyuzLxYzPzgOM0H0

# **ViT Transformers**
"""

pip install torch torchvision transformers

from google.colab import drive
drive.mount('/content/drive')

root_path = '/content/drive/MyDrive/insuldataset'
import os
PATH = os.path.join(os.path.dirname(root_path), "insuldataset")
print(PATH)

import os
import torch
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

# Define transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize to 224x224 for ViT
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize
])

# Load dataset
#dataset_dir = '/content/drive/MyDrive/insuldataset'  # Update this path
train_dataset = ImageFolder(os.path.join(PATH, 'Train'), transform=transform)
test_dataset = ImageFolder(os.path.join(PATH, 'Test'), transform=transform)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

"""# **Vision Transformer with Adadelta Optimizers**"""

from transformers import ViTForImageClassification

# Load the pre-trained Vision Transformer model
num_classes = len(train_dataset.classes)  # Number of classes in your dataset
vtadadeltamodel = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224-in21k", num_labels=num_classes)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vtadadeltamodel.to(device)

import torch.nn as nn
from torch.optim import Adadelta
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adadelta(vtadadeltamodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
vtadadeltamodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = vtadadeltamodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, average_precision_score
from sklearn.metrics import roc_auc_score
# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtadadeltamodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtadadeltamodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

vtadadeltamodel.eval()
correct = 0
total = 0

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtadadeltamodel(images).logits
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy: {100 * correct / total:.2f}%')

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtadadeltamodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtadadeltamodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, average_precision_score
from sklearn.metrics import roc_auc_score
# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtadadeltamodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtadadeltamodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

"""# **SGD optimizer for vision transformers**"""

from transformers import ViTForImageClassification

# Load the pre-trained Vision Transformer model
num_classes = len(train_dataset.classes)  # Number of classes in your dataset
vtsdgmodel = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224-in21k", num_labels=num_classes)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vtsdgmodel.to(device)

import torch.nn as nn
from torch.optim import SGD
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = SGD(vtsdgmodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
vtsdgmodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = vtsdgmodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, average_precision_score
from sklearn.metrics import roc_auc_score
# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtsdgmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtsdgmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtsdgmodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtsdgmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

"""# **AdaGrad Optimizer for Vision Transformers**"""

from transformers import ViTForImageClassification

# Load the pre-trained Vision Transformer model
num_classes = len(train_dataset.classes)  # Number of classes in your dataset
vtadagradmodel = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224-in21k", num_labels=num_classes)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vtadagradmodel.to(device)

import torch.nn as nn
from torch.optim import Adagrad
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adagrad(vtadagradmodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
vtadagradmodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = vtadagradmodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, average_precision_score
from sklearn.metrics import roc_auc_score
# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtadagradmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtadagradmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtadagradmodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtadagradmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

"""# **RMSProp Optimier for Vision Transformers**"""

from transformers import ViTForImageClassification

# Load the pre-trained Vision Transformer model
num_classes = len(train_dataset.classes)  # Number of classes in your dataset
vtrmsmodel = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224-in21k", num_labels=num_classes)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vtrmsmodel.to(device)

import torch.nn as nn
from torch.optim import RMSprop
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = RMSprop(vtrmsmodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
vtrmsmodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = vtrmsmodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, average_precision_score
from sklearn.metrics import roc_auc_score
# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtrmsmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtrmsmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtrmsmodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtrmsmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

"""# **Adam Optimizer for Vision Transformers**"""

from transformers import ViTForImageClassification

# Load the pre-trained Vision Transformer model
num_classes = len(train_dataset.classes)  # Number of classes in your dataset
vtadammodel = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224-in21k", num_labels=num_classes)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vtadammodel.to(device)

import torch.nn as nn
from torch.optim import Adam
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adam(vtadammodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
vtadammodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = vtadammodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, average_precision_score
from sklearn.metrics import roc_auc_score
# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtadammodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtadammodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtadammodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtadammodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

"""## **End ViT**"""

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

import torch
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import accuracy_score, confusion_matrix

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

vitinsul={'loss': [0.0941,0.0796,0.0268,0.0192,0.0115,0.0014,0.0009,0.0007,0.0006,0.0005],'accuracy': [0.9059,0.9204,0.9732,0.9808,0.9885,0.9986,0.9991,0.9993,0.9994,0.9995]}

# Plot training loss
import os
import numpy as np
import matplotlib.pyplot as plt
vitinsul={'loss': [0.0941,0.0796,0.0268,0.0192,0.0115,0.0014,0.0009,0.0007,0.0006,0.0005],'accuracy': [0.9059,0.9204,0.9732,0.9808,0.9885,0.9986,0.9991,0.9993,0.9994,0.9995]}
epochs_range = range(10)
plt.figure(figsize=(8,8))
plt.plot(epochs_range, vitinsul['accuracy'], label='ViT Accuracy')
plt.title('Training Accuracy', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.legend(loc="upper left")
plt.grid()
plt.show()

epochs_range = range(10)
plt.figure(figsize=(8,8))
plt.plot(epochs_range, vitinsul['loss'], label='ViT Loss',color='red')
plt.title('Training Loss', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend(loc="upper left")
plt.grid()
plt.show()

"""# SWIM **Transformers**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

class SwinTransformerBlock(nn.Module):
    def __init__(self, in_channels, out_channels, window_size=7, shift_size=0, num_heads=4):
        super(SwinTransformerBlock, self).__init__()
        self.attention = nn.MultiheadAttention(embed_dim=out_channels, num_heads=num_heads)
        self.norm1 = nn.LayerNorm(out_channels)
        self.norm2 = nn.LayerNorm(out_channels)
        self.ffn = nn.Sequential(
            nn.Linear(out_channels, out_channels * 4),
            nn.ReLU(),
            nn.Linear(out_channels * 4, out_channels)
        )
        self.window_size = window_size
        self.shift_size = shift_size

    def forward(self, x):
        # Apply window-based multi-head self-attention
        x = self.norm1(x)
        attn_output, _ = self.attention(x, x, x)
        x = x + attn_output  # Skip connection

        # Feed-forward network
        x = self.norm2(x)
        x_ffn = self.ffn(x)
        x = x + x_ffn  # Skip connection

        return x

class SwinTransformer(nn.Module):
    def __init__(self, num_classes, embed_dim=96, num_layers=4, num_heads=4):
        super(SwinTransformer, self).__init__()
        self.embed_dim = embed_dim

        self.patch_size = 4  # Example patch size
        self.num_layers = num_layers

        # Initial linear projection for patch embedding
        self.proj = nn.Linear(3 * self.patch_size * self.patch_size, embed_dim)

        # Swin Transformer blocks
        self.layers = nn.ModuleList([
            SwinTransformerBlock(embed_dim, embed_dim, num_heads=num_heads) for _ in range(num_layers)
        ])

        # Classifier
        self.classifier = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        # Patch embedding
        batch_size, _, height, width = x.size()
        x = x.view(batch_size, 3, height // self.patch_size, self.patch_size, width // self.patch_size, self.patch_size)
        x = x.permute(0, 2, 4, 1, 3, 5).reshape(batch_size, -1, 3 * self.patch_size * self.patch_size)
        x = self.proj(x)

        # Pass through Swin Transformer layers
        for layer in self.layers:
            x = layer(x)

        # Global average pooling
        x = x.mean(dim=1)
        x = self.classifier(x)

        return x

# Data preparation
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

#train_dataset = ImageFolder(root='path_to_your_dataset/train', transform=transform)
#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
train_dataset = ImageFolder(os.path.join(PATH, 'Train'), transform=transform)
test_dataset = ImageFolder(os.path.join(PATH, 'Test'), transform=transform)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Hyperparameters
num_classes = len(train_dataset.classes)
#num_classes = 10  # Adjust based on your dataset
batch_size = 32
num_epochs = 10
learning_rate = 0.001

# Initialize model, loss, and optimizer
swimmodel = SwinTransformer(num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(swimmodel.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    swimmodel.train()
    running_loss = 0.0

    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = swimmodel(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')

# Save the model
torch.save(swimmodel.state_dict(), 'swin_transformer_model.pth')

import os
import torch
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

# Define transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images to 224x224
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize
])

# Load dataset
#dataset_dir = '/path/to/dataset'  # Update this path
train_dataset = ImageFolder(os.path.join(PATH, 'Train'), transform=transform)
test_dataset = ImageFolder(os.path.join(PATH, 'Test'), transform=transform)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

pip install torch torchvision transformers

!pip install --upgrade transformers

from transformers import AutoImageProcessor, SwinModel
import torch
image_processor = AutoImageProcessor.from_pretrained("microsoft/swin-tiny-patch4-window7-224")
swimmodel = SwinModel.from_pretrained("microsoft/swin-tiny-patch4-window7-224")



# Load a pretrained Swin model and feature extractor
#model_name = "microsoft/swin-base-patch4-window7-224"
#microsoft/swin-tiny-patch4-window7-224
#swimmodel = SwinForImageClassification.from_pretrained(model_name)
#feature_extractor = SwinFeatureExtractor.from_pretrained(model_name)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
swimmodel.to(device)

import torch.nn as nn
from torch.optim import Adam
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adam(vtmodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
vtmodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = vtmodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch.nn as nn
from torch.optim import Adam
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adam(swimmodel.parameters(), lr=3e-4)

# Move the model to GPU if available
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#swimmodel.to(device)

# Training loop
num_epochs = 10
swimmodel.train()
train_losses = []

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = swimmodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    epoch_loss = running_loss / len(train_loader)
    train_losses.append(epoch_loss)  # Store the average loss for the epoch
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

swimmodel.eval()
correct = 0
total = 0

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = swimmodel(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy: {100 * correct / total:.2f}%')

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtmodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

vtmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = vtmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

# Plot training loss
plt.figure(figsize=(10, 5))
plt.plot(accuracy, color='silver', label='Accuracy')
plt.title('Training Accuracy', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.legend(loc="upper left")
plt.grid()
plt.show()

"""# **Adam Optimizer for DeiT (Data-efficient Image Transformers)**"""

pip install torch torchvision transformers

import os
import torch
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

# Define transformations for training and testing
transform_train = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize to the input size of Cait
    transforms.RandomHorizontalFlip(),  # Data augmentation
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load datasets
#dataset_dir = '/path/to/dataset'  # Update this path
train_dataset = ImageFolder(os.path.join(PATH, 'Train'), transform=transform_train)
test_dataset = ImageFolder(os.path.join(PATH, 'Test'), transform=transform_test)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

from transformers import DeiTForImageClassification, DeiTFeatureExtractor

# Load a pretrained DeiT model and feature extractor
model_name = "facebook/deit-base-distilled-patch16-224"
deitmodel = DeiTForImageClassification.from_pretrained(model_name)
feature_extractor = DeiTFeatureExtractor.from_pretrained(model_name)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
deitmodel.to(device)

import torch.nn as nn
from torch.optim import Adam
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adam(deitmodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
deitmodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = deitmodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

deitmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

deitmodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

"""CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"""

deitinsul={'loss': [0.0325,0.0319, 0.0064,0.0119,0.0201,0.0025,0.0000,0.0000,0.0000,0.0000],'accuracy': [1-0.0325,1-0.0319, 1-0.0064,1-0.0119,1-0.0201,1-0.0025,1-0.0000,1-0.0000,1-0.0000,1-0.0000]}

"""Epoch [1/10], Loss: 0.0325
100%|| 49/49 [03:41<00:00,  4.52s/it]
Epoch [2/10], Loss: 0.0319
100%|| 49/49 [03:36<00:00,  4.41s/it]
Epoch [3/10], Loss: 0.0064
100%|| 49/49 [03:39<00:00,  4.48s/it]
Epoch [4/10], Loss: 0.0119
100%|| 49/49 [03:39<00:00,  4.48s/it]
Epoch [5/10], Loss: 0.0201
100%|| 49/49 [03:37<00:00,  4.44s/it]
Epoch [6/10], Loss: 0.0025
100%|| 49/49 [04:34<00:00,  5.60s/it]
Epoch [7/10], Loss: 0.0000
100%|| 49/49 [04:38<00:00,  5.67s/it]
Epoch [8/10], Loss: 0.0000
100%|| 49/49 [03:41<00:00,  4.52s/it]
Epoch [9/10], Loss: 0.0000
100%|| 49/49 [03:38<00:00,  4.46s/it]Epoch [10/10], Loss: 0.0000
"""

# Plot training loss
import os
import numpy as np
import matplotlib.pyplot as plt
epochs_range = range(10)
plt.figure(figsize=(8,8))
plt.plot(epochs_range, deitinsul['accuracy'], label='DeiT Accuracy')
plt.title('Training Accuracy', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.legend(loc="upper left")
plt.grid()
plt.show()

epochs_range = range(10)
plt.figure(figsize=(8,8))
plt.plot(epochs_range, deitinsul['loss'], label='DeiT Loss',color='red')
plt.title('Training Loss', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend(loc="upper right")
plt.grid()
plt.show()

"""# **Adadelta for Deit**"""

from transformers import DeiTForImageClassification, DeiTFeatureExtractor

# Load a pretrained DeiT model and feature extractor
model_name = "facebook/deit-base-distilled-patch16-224"
deitadadeltamodel = DeiTForImageClassification.from_pretrained(model_name)
feature_extractor = DeiTFeatureExtractor.from_pretrained(model_name)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
deitadadeltamodel.to(device)

import torch.nn as nn
from torch.optim import Adadelta
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adadelta(deitadadeltamodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
deitadadeltamodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = deitadadeltamodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

deitadadeltamodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitadadeltamodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

deitadadeltamodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitadadeltamodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

"""# **SGD for DeiT**"""

from transformers import DeiTForImageClassification, DeiTFeatureExtractor

# Load a pretrained DeiT model and feature extractor
model_name = "facebook/deit-base-distilled-patch16-224"
deitmodel = DeiTForImageClassification.from_pretrained(model_name)
feature_extractor = DeiTFeatureExtractor.from_pretrained(model_name)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
deitmodel.to(device)

import torch.nn as nn
from torch.optim import Adam
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adam(deitmodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
deitmodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = deitmodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

deitmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

deitmodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score
# Calculate metrics
def calculate_metrics(cm):
    # True Positives, False Positives, False Negatives, True Negatives
    TP = np.diag(conf_matrix)  # True positives are on the diagonal
    FP = np.sum(conf_matrix, axis=0) - TP  # False positives
    FN = np.sum(conf_matrix, axis=1) - TP  # False negatives
    TN = np.sum(conf_matrix) - (FP + FN + TP)  # True negatives

    # Precision, Recall, Specificity
    precision = TP / (TP + FP + 1e-10)  # Avoid division by zero
    recall = TP / (TP + FN + 1e-10)
    specificity = TN / (TN + FP + 1e-10)

    # F1 Score
    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)

    # Overall Accuracy
    accuracy = np.sum(TP) / np.sum(cm)

    # Fowlkes-Mallows Index (FM)
    FM = np.sqrt(precision * recall)

    return precision, recall, accuracy, f1, specificity, FM

# Calculate metrics
precision, recall, accuracy, f1, specificity, FM = calculate_metrics(cm)

# Print results
print("Precision for each class:", precision)
print("Recall for each class:", recall)
print("Overall Accuracy:", accuracy)
print("F1 Score for each class:", f1)
print("Specificity for each class:", specificity)
print("Fowlkes-Mallows Index for each class:", FM)



"""AdaGrad for Deit"""

from transformers import DeiTForImageClassification, DeiTFeatureExtractor

# Load a pretrained DeiT model and feature extractor
model_name = "facebook/deit-base-distilled-patch16-224"
deitmodel = DeiTForImageClassification.from_pretrained(model_name)
feature_extractor = DeiTFeatureExtractor.from_pretrained(model_name)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
deitmodel.to(device)

import torch.nn as nn
from torch.optim import Adam
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adam(deitmodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
deitmodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = deitmodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

deitmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

deitmodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

"""RMSprop for DeiT"""

from transformers import DeiTForImageClassification, DeiTFeatureExtractor

# Load a pretrained DeiT model and feature extractor
model_name = "facebook/deit-base-distilled-patch16-224"
deitmodel = DeiTForImageClassification.from_pretrained(model_name)
feature_extractor = DeiTFeatureExtractor.from_pretrained(model_name)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
deitmodel.to(device)

import torch.nn as nn
from torch.optim import Adam
from tqdm import tqdm

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = Adam(deitmodel.parameters(), lr=3e-4)

# Training loop
num_epochs = 10
deitmodel.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = deitmodel(images).logits
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

deitmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

deitmodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()
# Calculate Accuracy
acc = accuracy_score(all_labels, all_predictions)

# Calculate Precision
precision = precision_score(all_labels, all_predictions, average='binary')

# Calculate Recall (Sensitivity)
recall = recall_score(all_labels, all_predictions, average='binary')

# Calculate F1 Score
f1 = f1_score(all_labels, all_predictions, average='binary')

# If it's binary classification, we can also calculate mAP by using average_precision_score
mAP = average_precision_score(all_labels, all_predictions) if len(np.unique(all_labels)) > 1 else None
# Return calculated metrics
print("Accuracy", acc)
print("Precision", precision)
print("Recall (Sensitivity)", recall)
print("F1 Score", f1)
print("mAP",mAP)

"""# Cross-**ViT**"""

pip install tensorflow tensorflow-addons

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa

# Define the CrossViT model (simplified version)
class CrossViT(layers.Layer):
    def __init__(self, num_classes):
        super(CrossViT, self).__init__()
        # Add your CrossViT layers here
        # This is a simplified version
        self.base_model = keras.applications.ViT(
            include_top=False,
            weights='imagenet',
            pooling='avg'
        )
        self.classifier = layers.Dense(num_classes, activation='softmax')

    def call(self, inputs):
        x = self.base_model(inputs)
        return self.classifier(x)

# Load and preprocess your dataset

#dataset_dir = '/path/to/dataset'  # Update this path
train_dataset = ImageFolder(os.path.join(PATH, 'Train'), transform=transform_train)
test_dataset = ImageFolder(os.path.join(PATH, 'Test'), transform=transform_test)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
def load_data(image_dir, img_size, batch_size):
    datagen = keras.preprocessing.image.ImageDataGenerator(
        validation_split=0.2,
        rescale=1./255
    )

    train_generator = datagen.flow_from_directory(
        image_dir,
        target_size=(img_size, img_size),
        batch_size=batch_size,
        class_mode='categorical',
        subset='training'
    )

    test_generator = datagen.flow_from_directory(
        image_dir,
        target_size=(img_size, img_size),
        batch_size=batch_size,
        class_mode='categorical',
        subset='validation'
    )

    return train_generator, validation_generator

# Parameters
image_directory = os.path.join(PATH, 'Train')
image_size = 224  # Size of input images
batch_size = 32
num_classes = len(tf.keras.utils.list_dir(image_directory))  # Adjust based on your dataset

# Load data
train_data, test_data = load_data(image_directory, image_size, batch_size)

# Build and compile the model
model = CrossViT(num_classes)
model.compile(
    optimizer=keras.optimizers.Adam(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model
model.fit(
    train_data,
    validation_data=val_data,
    epochs=10  # Adjust the number of epochs
)

# Save the model
model.save('crossvit_model.h5')

import os
import torch
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

# Define transformations for training and testing
transform_train = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize to the input size of Cait
    transforms.RandomHorizontalFlip(),  # Data augmentation
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load datasets
#dataset_dir = '/path/to/dataset'  # Update this path
train_dataset = ImageFolder(os.path.join(PATH, 'Train'), transform=transform_train)
test_dataset = ImageFolder(os.path.join(PATH, 'Test'), transform=transform_test)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
from torchvision import models

class CrossViT(nn.Module):
    def __init__(self, num_classes):
        super(CrossViT, self).__init__()
        # Example components; replace with your actual CrossViT architecture
        self.patch_size = 16
        self.embed_dim = 768
        self.num_heads = 12
        self.num_layers = 12

        # Vision Transformer layers
        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.embed_dim, nhead=self.num_heads)
        self.classifier = nn.Linear(self.embed_dim, num_classes)

    def forward(self, x):
        # Tokenization and embedding steps
        batch_size, _, height, width = x.size()
        x = x.view(batch_size, 3, height // self.patch_size, self.patch_size, width // self.patch_size, self.patch_size)
        x = x.permute(0, 2, 4, 1, 3, 5).reshape(batch_size, -1, 3 * self.patch_size * self.patch_size)

        # Pass through transformer
        x = self.transformer_layer(x)
        x = x.mean(dim=1)  # Global average pooling
        x = self.classifier(x)
        return x

# Hyperparameters
num_classes = 2  # Adjust based on your dataset
batch_size = 32
num_epochs = 10
learning_rate = 0.001

# Data preparation
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])
# Define transformations for training and testing
transform_train = transforms.Compose([
    transforms.Resize((128,128)),  # Resize to the input size of Cait
    transforms.ToTensor(),
])

transform_test = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

# Load datasets
#dataset_dir = '/path/to/dataset'  # Update this path
train_dataset = ImageFolder(os.path.join(PATH, 'Train'), transform=transform_train)
test_dataset = ImageFolder(os.path.join(PATH, 'Test'), transform=transform_test)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
#train_dataset = ImageFolder(root='path_to_your_dataset/train', transform=transform)
#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Initialize model, loss, and optimizer


cvitmodel = CrossViT(num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(cvitmodel.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    cvitmodel.train()
    running_loss = 0.0

    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = cvitmodel(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')

     # Testing
    cvitmodel.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for images, labels in test_loader:
            outputs = cvitmodel(images)
            test_loss += criterion(outputs, labels).item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()

    test_accuracy = correct / len(test_loader)
    print(f'Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}')


# Save the model
torch.save(cvitmodel.state_dict(), 'crossvit_model.pth')



import torch
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []
cvitmodel.eval()

with torch.no_grad():
    for images, labels in train_loader:
#       images, labels = images.to(device), labels.to(device)
        outputs = deitmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

accuracy = accuracy_score(all_labels, all_predictions)
print(f"Accuracy on the test dataset: {accuracy:.2f}")

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store true labels and predictions
all_labels = []
all_predictions = []

cvitmodel.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = deitmodel(images).logits
        _, predicted = torch.max(outputs.data, 1)

        # Collect true labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Convert lists to numpy arrays for scikit-learn
all_labels = np.array(all_labels)
all_predictions = np.array(all_predictions)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=train_dataset.classes,
            yticklabels=train_dataset.classes)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

"""Epoch [1/10], Loss: 0.8311
Epoch [2/10], Loss: 0.6802
Epoch [3/10], Loss: 0.6613
Epoch [4/10], Loss: 0.6801
Epoch [5/10], Loss: 0.6725
Epoch [6/10], Loss: 0.6547
Epoch [7/10], Loss: 0.6381
Epoch [8/10], Loss: 0.6477
"""

cvitinsul={'loss': [0.0325,0.0319, 0.0064,0.0119,0.0201,0.0025,0.0000,0.0000,0.0000,0.0000],'accuracy': [1-0.0325,1-0.0319, 1-0.0064,1-0.0119,1-0.0201,1-0.0025,1-0.0000,1-0.0000,1-0.0000,1-0.0000]}

# Plot training loss
import os
import numpy as np
import matplotlib.pyplot as plt
epochs_range = range(10)
plt.figure(figsize=(8,8))
plt.plot(epochs_range, deitinsul['accuracy'], label='DeiT Accuracy')
plt.title('Training Accuracy', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.legend(loc="upper left")
plt.grid()
plt.show()

epochs_range = range(10)
plt.figure(figsize=(8,8))
plt.plot(epochs_range, deitinsul['loss'], label='DeiT Loss',color='red')
plt.title('Training Loss', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend(loc="upper right")
plt.grid()
plt.show()